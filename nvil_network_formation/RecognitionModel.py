import torch
import settings
import torch.nn as nn
import numpy as np
import math

USE_CUDA = torch.cuda.is_available()
dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor
class Variable(torch.autograd.Variable):
    def __init__(self, data, *args, **kwargs):
        if USE_CUDA:
            data = data.cuda()
        super(Variable, self).__init__(data, *args, **kwargs)

class recognition_RNN(nn.Module):
    def __init__(self, input_size = settings.number_of_features, hidden_size = settings.n_hidden,
                 num_layers = settings.NUM_LAYERS, output_size = settings.OUTPUT_SIZE,nCUnits = 4):
        super(recognition_RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.rec = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=False,
        )#batch_first=False the first dimension is Time, the second dimension is Batch

        self.lin1 = nn.Linear(hidden_size, nCUnits)
        # self.lin1 = nn.Linear(yDim, nCUnits)
        self.nonlinearity1 = nn.LeakyReLU(0.1)
        self.lin2 = nn.Linear(nCUnits, output_size)
        self.nonlinearity2 = nn.Softmax()
        # self.training_losses = []

    def forward(self, input, hidden):
        output_r, _ = self.rec(input, hidden)  # the recurrent units output
        output_n = self.nonlinearity1(output_r[-1])  # non-linearity after the recurrent units
        output_l = self.lin2(self.lin1(output_n))  # second linear layer
        output = self.nonlinearity2(output_l)  # final non-linearity is a softmax
        # print('self.lin2.bias:', self.lin2.bias)
        return output

    def initHidden(self):
        return (Variable(torch.zeros(self.num_layers, 1, self.hidden_size).type(dtype)),
                Variable(torch.zeros(self.num_layers, 1, self.hidden_size).type(dtype)))

    def getSamplePrediction(self,torchSample):
        hidden0 = self.initHidden()
        # print(torchSample)
        if isinstance(torchSample,list) or isinstance(torchSample,tuple):
            feature = torchSample[1]
        else:
            feature = torchSample
        output_label = self.__call__(Variable(torch.from_numpy(np.asarray(feature)).type(dtype)), hidden0)
        _, prediction = torch.topk(output_label, 1)
        return prediction.data[0][0], np.exp(output_label.data[0][prediction.data[0][0]])

    def getSampleOutput(self,torchSample):
        hidden0 = self.initHidden()
        # print(torchSample)
        if isinstance(torchSample,list) or isinstance(torchSample,tuple):
            feature = torchSample[1]
        else:
            feature = torchSample
        output = self.__call__(Variable(torch.from_numpy(np.asarray(feature)).type(dtype)), hidden0)

        return output



class NetworkFormationRecognition(recognition_RNN):

    def __init__(self, number_of_classes):
        '''
        h = Q_phi(x|y), where phi are parameters, x is our latent class, and y are data
        '''
        super(NetworkFormationRecognition, self).__init__()
        self.number_of_classes=number_of_classes


    def getSampleDist(self, Y):
        # Y is generated by sampleXY in the generative model.
        # note that each sample is a dictionary with two keys: 'degrees' and 'network_time_series', only the degrees is
        # supplied as input to the recognition network
        hidden0 = self.initHidden()
        input_degrees = Y['degrees'].unsqueeze(0)
        input_degrees = input_degrees.permute(1, 0, 2)
        # print('input_degrees',input_degrees)
        self.h = self.__call__(Variable(input_degrees), hidden0)
        # self.h = self.forward(input_degrees)  # this is the (classification) neural network output,
        # posterior probabilities for each class
        # print(self.h)
        pi = np.asarray(torch.clamp(self.h, 0.001, 0.999).data)
        # print(pi)
        pi = (1/pi.sum(axis=1))[:, np.newaxis]*pi #enforce normalization (undesirable; for numerical stability)
        return pi

    def getSample(self, Y):
        pi = self.getSampleDist(Y)
        # print('pi in recognition', pi)
        x_vals = np.zeros([pi.shape[0], self.number_of_classes])
        theta_vals = np.zeros([pi.shape[0]])
        for ii in range(pi.shape[0]):
            # print(pi[ii])
            # print(np.random.multinomial(1, pi[ii], size=1))
            x_vals[ii,:] = np.random.multinomial(1, pi[ii], size=1)
            # print('x_vals[ii,:] in recognition: ' , x_vals[ii,:])
            # b_val = x_vals[ii,:]
            # print('b_vals:', b_vals)
            class_label = x_vals[ii,:].nonzero()[0][0]  # the index of the chosen class
            # print('class_label:', class_label)
            mean_of_gaussain = settings.class_values[class_label]
            theta_vals[ii] = np.random.normal(mean_of_gaussain, 1)
            # print('theta_vals[ii]', theta_vals[ii])
            if theta_vals[ii] < 0:
                theta_vals[ii] = 0
            # print(theta_vals)
        # print(Variable(torch.FloatTensor(theta_vals)))
        # print(Variable(torch.FloatTensor(x_vals.astype(bool) * 1.0)))
        return Variable(torch.from_numpy(theta_vals).type(dtype)) #Variable(torch.FloatTensor(x_vals.astype(bool) * 1.0))

    def normal_pdf(self, value, mean=0, std=1):
        # print(value.type(torch.FloatTensor))
        # print('value', value.data)
        # print('value',value)
        # print('value_squared:',-value.sub(mean)**2)
        gaussian_exponenet = torch.div(-value.sub(mean)**2, 2*std**2)#.type(torch.FloatTensor)
        # print('gaussian_exponenet:', gaussian_exponenet)
        density_evaluated = torch.div(torch.exp(gaussian_exponenet), math.sqrt(2 * math.pi) * std)
        # print('z', z)
        # z = value/math.sqrt(2)
        # print('z',z)
        return density_evaluated  # 0.5 * (1 + torch.erf(z.type(torch.FloatTensor)))
        #0.5 * (1 + torch.erf(torch.FloatTensor([value /math.sqrt(2)])))

    def evalLogDensity(self, theta_samp, Y):
        ''' We assume each sample is a single multinomial sample from the latent h, so each sample is an integer class.'''
        hidden0 = self.initHidden()
        input_degrees = Y['degrees'].unsqueeze(0)
        input_degrees = input_degrees.permute(1, 0, 2)
        self.h = self.__call__(Variable(input_degrees), hidden0)

        gaussian_probabilities = []
        for i in range(settings.number_of_classes):
            # print(self.h.data[i])
            gaussian_probabilities += [self.normal_pdf(theta_samp, mean=settings.class_values[i], std=1)]
            # print('gaussian_probabilities:', gaussian_probabilities)


        gaussian_probabilities = torch.squeeze(torch.stack(gaussian_probabilities))
        # print('self.h:', self.h)
        # print('gaussian_probabilities:',gaussian_probabilities)
        total_probability = torch.squeeze(torch.dot(gaussian_probabilities,self.h))
        # print('total_probability:',total_probability)
        # print(torch.log(total_probability))
        # print(torch.log(torch.sum(self.h*theta_samp, 1)))
        return torch.log(total_probability)  # torch.log(torch.sum(self.h*theta_samp, 1))
